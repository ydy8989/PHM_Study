# 7월 5주차

https://www.phmsociety.org/events/conference/phm/18/data-challenge

### Contents

1. Data
2. Feature engineering
3. Model
   1. LSTM
   2. AutoEncoder
   3. ETC) Mask-LSTM, RUL-Net...
4. Weibull Distribution



### 1. Data

![sensor_data](https://user-images.githubusercontent.com/38639633/62459789-3d7ed380-b7bb-11e9-9b84-1098e361ecfb.PNG)

> 센서데이터 약 300만 X 24 columns

![ttf](https://user-images.githubusercontent.com/38639633/62459865-6606cd80-b7bb-11e9-8b6c-871fba6c2402.PNG)

> RUL(Remaining Useful Life) 데이터 약 300만 X 4columns



### 2. Feature engineering

**Two methods**

1. <u>*for recurrent prediction*</u>
2. <u>for Classification</u>
3. <u>Extra feature engineering</u>



**for Recurrent prediction**

- 아래 gif에서 `*`은 Fault 가 발생한 시점.

- 윈도우를 슬라이딩 시키며 윈도우 하나 당 레이블 하나를 매칭시킨다.

  - 여기서 레이블은 해당 윈도우를 통과하는 순간, 기계의 남은 수명(RUL)으로 레이블링

  ![](http://i.imgur.com/hkpM09J.gif)

**for Classification**

- 해당 윈도우가 fault를 포함하는지 여부에 따라 0과 1로 레이블링한 모습
- 다양한 방식으로 레이블링이 가능함
  1. 윈도우 구간과 에러 발생 지점이 얼마나 남았는지로 차등 레이블링
  2. 윈도우가 걸친 부분에서의 시간으로 레이블링 
  3. 기타 등등...

![](http://i.imgur.com/osjA1uZ.gif)

- time series에서 단일 시간에 대한 레이블링 후 예측이 아닌, <u>*범위 시간*</u>(윈도우 - 아래 그림에서 `빨간 박스`)에 대한 <u>*레이블링*</u> 후 예측 진행

  

![dataexample](https://user-images.githubusercontent.com/38639633/62460686-a5361e00-b7bd-11e9-8428-00e7d3592c55.png)



**Extra preprocessing**

> 데이터의 특성에 맞는 별도의 세부 전처리 과정이 필요함.
>
> 1. Lot 컬럼과 stage 컬럼을 무시하고 학습 >> Lot 컬럼과 stage 컬럼들로 분류한 새로운 데이터 프레임의 주기성을 발견함
>
> <video src="C:\Users\hbee\Videos\Captures\Spyder (Python 3.6) 2019-08-05 20-30-29.mp4"></video>
>
> 2. 중간마다 나오는 다른 양상을 띄는 주기들은 거의 에러가 발생한 지점을 포함할 가능성이 높음을 발견함.
>
> <video src="C:\Users\hbee\Videos\Captures\Spyder (Python 3.6) 2019-08-05 20-40-37.mp4"></video>
>
> 3. 이 관찰을 통해, 각 데이터 프레임의 Lot과 stage 별로  정규화해야겠다고 생각함.
> 4. 이상치를 확실하게 제거하지 않았을 경우에, 더 심각하게 왜곡되는 모습을 관찰할 수 있었음.



**ISSUE**

- [x] ~~데이터 프레임의 Lot과 stage별로 정규화 후 합친 뒤 윈도우를 자르는 과정에서 인덱싱 문제 발생~~
- [ ] 전체 데이터의 길이가 매우 긴 데 반해, Lot,stage로 데이터프레임을 등분할 경우, 너무 잘게 잘라지는 현상. >> 윈도우 1개와 레이블링 1개의 매칭에 대한 어려움 발생 ----(해결중...)

---

### 3. Model

**3.1. LSTM**

​	![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQSJYj7z9Sxx-YW4vzMwQaggE2Z0XXzc1OjiLK4KOkRF2KC4wsk0A)

​	

**3.2. AutoEncoder**

![](https://github.com/ydy8989/PHM_Study/blob/master/pic/lstmauto.png?raw=true)



### 4. Weibull Distribution

- 모델과 데이터의 특성상, 분류든, 예측이든 '정확함'에 대한 정의가 모호함을 느낌
- PHM 데이터에 관한 스코어 metric과 예측에 대한 신뢰도 부여 방식에 대한 내용을 학습함.

**Weibull 분포?**

![](https://imgur.com/maUHyto.gif)

> `알파`와 `베타` 두개의 파라미터로 움직이는 분포로써, 부품의 수명 추정 분석에 자주 사용되는 분포이다. 

- 아래 그림과 같이, 일반적인 LSTM를 사용한 뒤, FCN 단계에서 2개의 feature를 뽑는다. 
- 2 개의 feature 중 하나는 exponential 함수에, 다른 하나는 sigmoid 혹은 softplus 함수에 넣고, 그 두 값을 통해, 분포를 추정한다. 

![2](https://user-images.githubusercontent.com/38639633/62464008-753f4880-b7c6-11e9-961b-711f18e1bb87.PNG)

**Architecture with likelihood fn.**



![](http://i.imgur.com/gEAoMQ4.png)

> 1. Input x를 LSTM을 통과시킨다.
> 2. LSTM cell의 output layer에 베이불(Weibull) 분포의 파라미터 두 개를 만들어내는 likelyhood fn을 붙인다.
> 3. likelihood를 통과한 값 `알파`와 `베타`로 분포를 만들고, <u>*해당 분포에서 다시 표본을 추출한 값이*</u> 예측값이 된다. 

---

**실제 데이터를 통한 예측 방법**

1. 위의 방식을 통해 `윈도우`가 이동함에 따라 바뀌는 분포를 주목한다. 
2. 분포의 최대치에 가까워질 수록, 이상 임박.
3. 데이터를 알고있는 현재 시점에서 분포의 최대값이 나올 것 같은 지점에서 설비를 점검.

![3](https://user-images.githubusercontent.com/38639633/62464786-7f624680-b7c8-11e9-87b8-f3a9a1a043d4.PNG)
![4](https://user-images.githubusercontent.com/38639633/62464787-7f624680-b7c8-11e9-896f-4c5074ab4f86.PNG)

---

---

**Turbofan Data.**

![pred_regress](https://user-images.githubusercontent.com/38639633/62465071-057e8d00-b7c9-11e9-97b4-ebb085eebba6.png)
![pred_regress2](https://user-images.githubusercontent.com/38639633/62465072-06172380-b7c9-11e9-9b6c-76e4f76a5253.png)

### reference

1. github
   1. mad-gan - 깃
   2. gan-ad - 깃
      1. 논문 : https://arxiv.org/pdf/1809.04758.pdf
      2. 블로그 : https://data-newbie.tistory.com/129?category=686943
   3. 

2. turbofan data

   1. 논문 : https://arxiv.org/pdf/1810.05644.pdf cnn + lstm **RUL-Net**

      1. Architecture :

         ![1564374785809](C:\Users\hbee\AppData\Roaming\Typora\typora-user-images\1564374785809.png)

      2. git search : RUL net

   2. 논문2 : https://arxiv.org/pdf/1709.01073.pdf predicting remaining useful life using time series embeddings based on recurrent neural networks

3. Credit card data

   1. autoencoder : https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd

4. weibull

   1. 베이스 : https://datasciencevademecum.wordpress.com/2018/02/06/deep-time-to-failure-predictive-maintenance-using-rnns-and-weibull-distributions/
   2. 드라이브 피피티 : https://docs.google.com/presentation/d/1H_TK9eQCMGTcslc4AnMCNTUskWIYcJAxsV18ac-fIqM/edit#slide=id.g23e42a43d9_0_358
   3. wtte-rnn 블로그 : https://ragulpr.github.io/
   4. 블로그2 : https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/
   5. github deep-ttf : https://github.com/gm-spacagna/deep-ttf
   6. https://github.com/ragulpr/wtte-rnn
   7. https://github.com/daynebatten/keras-wtte-rnn

5. https://cloud.google.com/blog/products/data-analytics/a-process-for-implementing-industrial-predictive-maintenance-part-ii : 구글클라우드

6. Deep Convolutional and LSTM Recurrent
   Neural Networks for Multimodal Wearable
   Activity Recognitions논문 : [http://sro.sussex.ac.uk/id/eprint/59271/1/2016%20-%20J%20-%20Ordonez%20%20-%20Deep%20Convolutional%20and%20LSTM%20Recurrent%20Neural%20Networks%20for%20Multimodal%20Wearable%20Activity%20Recognition%20%28Sensors%2C%202016%29.pdf](http://sro.sussex.ac.uk/id/eprint/59271/1/2016 - J - Ordonez  - Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition (Sensors%2C 2016).pdf)

   

---

### TODO

**End to End Processing**

- [x] 데이터 로드
- [x] Lot과 stage 하나하나 잘라서 Normalization
- [x] Outlier 제거 - 분포의 +- 3%의 양 사이드를 제거
- [x] 윈도우로 자르되, 에러 하나라도 끼면 다 1 아니면 0으로 labeling
- [x] deep-ttf 확인하기
- [x] weibull distribution + RNN
- [x] 그리고 wtte-rnn 공부

---



